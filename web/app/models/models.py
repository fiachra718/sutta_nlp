# models.py
from datetime import datetime
from typing import List, Literal, Optional, ClassVar

from pydantic import BaseModel, Field, field_validator, model_validator
from hashlib import md5
import hashlib
import unicodedata
import json
import uuid

from psycopg.types.json import Json

from ..db import db
from .manager import Manager


CANDIDATE_COLUMNS = ("id", "source_identifier", "source_verse_num", "text", "text_hash", "entities")
TRAINING_COLUMNS = ("id", "text", "text_hash", "spans", "spans_hash", "source", "from_file", "created_at")


def _candidate_row_processor(row):
    data = {col: row.get(col) for col in CANDIDATE_COLUMNS}
    data["entities"] = data.get("entities") or []
    return data


def _training_row_processor(row):
    data = {col: row.get(col) for col in TRAINING_COLUMNS}
    data["spans"] = data.get("spans") or []
    return data


Label = Literal["PERSON", "GPE", "LOC", "NORP", "EVENT"]


class SuttaVerse(BaseModel):
    identifier: str          # e.g., "mn.001.than"
    verse_num: int
    text: str                # NFC normalized; what you render & annotate
    text_hash: str           # md5 of normalized text


class Span(BaseModel):
    start: int = Field(ge=0)
    end: int = Field(gt=0)
    label: Label

    @model_validator(mode="after")
    def check_offsets(self):
        if self.end <= self.start:
            raise ValueError("span.end must be > span.start")
        if self.start < 0:
            raise ValueError("span start must be > 0")
        return self


class CandidateDoc(BaseModel):
    id: Optional[int] = None                    # autogenerated by Postgres
    source_identifier: Optional[str] = None
    source_verse_num: Optional[int] = None
    text: str                                   # exact review text (same as SuttaVerse.text)
    text_hash: Optional[str] = None                   # gen md5Sum
    entities: List[tuple[str, str]] = Field(default_factory=list)

    objects: ClassVar[Manager] = Manager(
        table="candidates",
        columns=CANDIDATE_COLUMNS,
        row_processor=_candidate_row_processor,
    )

    @model_validator(mode="before")
    @classmethod
    def normalize_and_hash(cls, data):
        if not isinstance(data, dict):
            return data
        txt = data.get("text", "")
        # Normalize once so offsets/lengths are stable across pipeline
        norm = unicodedata.normalize("NFC", txt) if isinstance(txt, str) else txt
        if isinstance(norm, str):
            data = {**data, "text": norm}
            if not data.get("text_hash"):
                data["text_hash"] = md5(norm.encode("utf-8")).hexdigest()
        return data

    @model_validator(mode="after")
    def check_hash(self):
        # If caller passed a hash, ensure it matches the normalized text
        expected = md5(self.text.encode("utf-8")).hexdigest()
        if self.text_hash and self.text_hash != expected:
            raise ValueError("text_hash does not match normalized text")
        # ensure entities are exactly 2-tuples
        for lab, txt in self.entities:
            if not isinstance(lab, str) or not isinstance(txt, str):
                raise ValueError("entities must be (label, text) pairs of strings")
        return self


class TrainingDoc(BaseModel):
    id: Optional[str] = None
    text: str
    text_hash: Optional[str] = None
    spans: list[Span]        # human-reviewed, overlap-free, slice-checked
    spans_hash: Optional[str] = None
    source: Optional[str] | None = None
    from_file: Optional[str] | None = None
    created_at: Optional[datetime] = None
    objects: ClassVar[Manager] = Manager(
        table="gold_training",
        columns=TRAINING_COLUMNS,
        row_processor=_training_row_processor,
        save_handler=lambda manager, doc, **kwargs: _save_training_doc(manager, doc, **kwargs),
    )

    @model_validator(mode="before")
    @classmethod
    def normalize_text(cls, data):
        if not isinstance(data, dict):
            return data
        text = data.get("text")
        if isinstance(text, str):
            norm = unicodedata.normalize("NFC", text)
            data = {**data, "text": norm}
            if not data.get("text_hash"):
                data["text_hash"] = md5(norm.encode("utf-8")).hexdigest()
        return data

    @field_validator("spans")
    @classmethod
    def validate_span_bounds(cls, spans, info):
        text = info.data.get("text") or ""
        length = len(text)
        if not spans:
            raise ValueError("At least one span is required")
        for idx, span in enumerate(spans, start=1):
            if span.end > length:
                raise ValueError(f"Span {idx} extends past text length {length}")
        spans_sorted = sorted(spans, key=lambda s: (s.start, s.end))
        for prev, curr in zip(spans_sorted, spans_sorted[1:]):
            if curr.start < prev.end:
                raise ValueError(f"Spans {prev} and {curr} overlap")
        return spans

    @model_validator(mode="after")
    def compute_hashes(self):
        computed_text_hash = md5(self.text.encode("utf-8")).hexdigest()
        if self.text_hash and self.text_hash != computed_text_hash:
            raise ValueError("text_hash does not match normalized text")
        self.text_hash = computed_text_hash

        canonical = self.canonical_spans()
        computed_spans_hash = hashlib.sha256(
            json.dumps(canonical, separators=(",", ":")).encode("utf-8")
        ).hexdigest()
        if self.spans_hash and self.spans_hash != computed_spans_hash:
            raise ValueError("spans_hash does not match canonical spans")
        self.spans_hash = computed_spans_hash
        return self

    def canonical_spans(self) -> list[dict[str, int | str]]:
        spans_sorted = sorted(self.spans, key=lambda s: (s.start, s.end, s.label))
        return [
            {"start": int(span.start), "end": int(span.end), "label": span.label}
            for span in spans_sorted
        ]

    def to_record(self, *, source: Optional[str] = None, from_file: Optional[str] = None) -> dict:
        if not self.id:
            self.id = f"manual:{uuid.uuid4().hex[:12]}"
        source_value = source or self.source or "manual"
        self.source = source_value
        if from_file is not None:
            self.from_file = from_file
        record = {
            "id": self.id,
            "text": self.text,
            "text_hash": self.text_hash,
            "spans": Json(self.canonical_spans()),
            "spans_hash": self.spans_hash,
            "source": source_value,
            "from_file": self.from_file,
        }
        return record


def _save_training_doc(manager, doc, *, source: Optional[str] = None, from_file: Optional[str] = None):
    if not isinstance(doc, TrainingDoc):
        doc = TrainingDoc.model_validate(doc)

    record = doc.to_record(source=source, from_file=from_file)

    result = db.save_training_record(record, dsn=manager.dsn_value)
    result["doc"] = doc
    return result


# sanity check
if __name__ == "__main__":
    # try loading a candidate doc
    data = {"text":"I heard thus. Once the Blessed One, while wandering in the Kosala country",
            "entities": [["PERSON", "Blessed One"], ["GPE", "Kosala"]] }
    

    doc = CandidateDoc.model_validate(data)
    print(doc.model_dump())


    # try loading a training doc
    record = {
            "text": "This Makkhali Gosala...",
        "spans": [
            {"start": 5, "end": 20, "label": "PERSON", "text": "Makkhali Gosala"}
        ]
    }
    doc = TrainingDoc.model_validate(record)
    print(doc.spans[0].label)   # PERSON
    print(doc.text)
    print(doc.model_dump())   # prints record + hash
