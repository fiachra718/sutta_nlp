# models.py
from datetime import datetime
from typing import List, Literal, Optional, ClassVar

from pydantic import BaseModel, Field, field_validator, model_validator
from hashlib import md5
import hashlib
import unicodedata
import json
import uuid
import re

from psycopg.types.json import Json

from ..db import db
from .manager import Manager


CANDIDATE_COLUMNS = ("id", "source_identifier", "source_verse_num", "text", "text_hash", "entities")
TRAINING_COLUMNS = ("id", "text", "text_hash", "spans", "spans_hash", "source", "from_file", "created_at")


def _candidate_row_processor(row):
    data = {col: row.get(col) for col in CANDIDATE_COLUMNS}
    data["entities"] = data.get("entities") or []
    return data


def _training_row_processor(row):
    data = {col: row.get(col) for col in TRAINING_COLUMNS}
    data["spans"] = data.get("spans") or []
    return data


Label = Literal["PERSON", "GPE", "LOC", "NORP", "EVENT", "UNIT"]


VERSE_COLUMNS = ("identifier", 
                 "verse_num", 
                 "text", 
                 "text_hash", 
                 "nikaya",
                 "vagga", 
                 "book_number", 
                 "translator", 
                 "title", 
                 "subtitle"
            )
QUOTE_CHARS = "\"'“”‘’‹›«»"
TRAILING_WRAP_CHARS = QUOTE_CHARS + ")]}›»”’"
SENTENCE_BOUNDARY_CHARS = ".!?;:…—–-"
TITLECASE_PATTERN = re.compile(r"\b[A-Z][a-z]+(?:[-'][A-Za-z]+)?\b")
TITLECASE_STOPWORDS = {
    "and", "or", "but", "nor", "for", "so", "yet",
    "the", "a", "an", "this", "that", "these", "those",
    "now", "then", "thus", "therefore", "because", "before", "after",
    "when", "while", "where", "here", "there", "why", "what", "which", "whose",
    "once", "with", "without", "within", "upon",
}


def _verse_row_processor(row):
    text = row.get("text") or ""
    return {
        "identifier": row["identifier"],
        "verse_num": row["verse_num"],
        "text": text,
        "text_hash": row.get("text_hash") or md5(text.encode("utf-8")).hexdigest(),
        "nikaya": row.get("nikaya"),
        "vagga": row.get("vagga"),
        "book_number": row.get("book_number"),
        "translator": row.get("translator"),
        "title": row.get("title"),
        "subtitle": row.get("subtitle"),
    }


def _clean_verse_text(text: str) -> str:
    stripped = (text or "").strip()
    return stripped.strip(QUOTE_CHARS).strip()


def _has_internal_titlecase(text: str) -> bool:
    if not text:
        return False
    for match in TITLECASE_PATTERN.finditer(text):
        start = match.start()
        word = match.group(0)
        lower = word.lower()
        if word.isupper() or lower in TITLECASE_STOPWORDS:
            continue
        preceding = text[:start].rstrip()
        if not preceding:
            continue
        preceding = preceding.rstrip(TRAILING_WRAP_CHARS).rstrip()
        if not preceding:
            continue
        last_char = preceding[-1]
        if last_char in SENTENCE_BOUNDARY_CHARS:
            continue
        return True
    return False


class SuttaVerse(BaseModel):
    identifier: str          # e.g., "mn.001.than"
    verse_num: int
    text: str                # NFC normalized; what you render & annotate
    text_hash: str           # md5 of normalized text
    translator: Optional[str] | None = None
    nikaya: Optional[str] | None = None
    vagga: Optional[str] | None = None
    book_number: Optional[str] | None = None
    title: Optional[str] | None = None
    subtitle: Optional[str] | None = None

    objects: ClassVar[Manager] = Manager(
        table="ati_suttas",
        columns=VERSE_COLUMNS,
        row_processor=_verse_row_processor,
    )

    @classmethod
    def random_with_titlecase(cls, *, max_attempts: int = 10):
        for _ in range(max_attempts):
            row = cls.objects.random_sutta_verse()
            if not row:
                break

            text = _clean_verse_text(row.get("verse_text") or "")
            if not text or not _has_internal_titlecase(text):
                continue

            verse_num = row.get("verse_num")
            identifier = row.get("identifier")
            if verse_num is None or identifier is None:
                continue

            return cls(
                identifier=identifier,
                verse_num=int(verse_num),
                text=text,
                text_hash=md5(text.encode("utf-8")).hexdigest(),
            )

        return None


class Span(BaseModel):
    start: int = Field(ge=0)
    end: int = Field(gt=0)
    label: Label
    text: str

    @model_validator(mode="after")
    def check_offsets(self):
        if self.end <= self.start:
            raise ValueError("span.end must be > span.start")
        if self.start < 0:
            raise ValueError("span start must be > 0")
        return self


class CandidateDoc(BaseModel):
    id: Optional[int] = None                    # autogenerated by Postgres
    source_identifier: Optional[str] = None
    source_verse_num: Optional[int] = None
    text: str                                   # exact review text (same as SuttaVerse.text)
    text_hash: Optional[str] = None                   # gen md5Sum
    entities: List[tuple[str, str]] = Field(default_factory=list)

    objects: ClassVar[Manager] = Manager(
        table="candidates",
        columns=CANDIDATE_COLUMNS,
        row_processor=_candidate_row_processor,
    )

    @model_validator(mode="before")
    @classmethod
    def normalize_and_hash(cls, data):
        if not isinstance(data, dict):
            return data
        txt = data.get("text", "")
        # Normalize once so offsets/lengths are stable across pipeline
        norm = unicodedata.normalize("NFC", txt) if isinstance(txt, str) else txt
        if isinstance(norm, str):
            data = {**data, "text": norm}
            if not data.get("text_hash"):
                data["text_hash"] = md5(norm.encode("utf-8")).hexdigest()
        return data

    @model_validator(mode="after")
    def check_hash(self):
        # If caller passed a hash, ensure it matches the normalized text
        expected = md5(self.text.encode("utf-8")).hexdigest()
        if self.text_hash and self.text_hash != expected:
            raise ValueError("text_hash does not match normalized text")
        # ensure entities are exactly 2-tuples
        for lab, txt in self.entities:
            if not isinstance(lab, str) or not isinstance(txt, str):
                raise ValueError("entities must be (label, text) pairs of strings")
        return self


class TrainingDoc(BaseModel):
    id: Optional[str] = None
    text: str
    text_hash: Optional[str] = None
    spans: list[Span] = Field(default_factory=list)
    spans_hash: Optional[str] = None
    source: Optional[str] | None = None
    from_file: Optional[str] | None = None
    created_at: Optional[datetime] = None
    objects: ClassVar[Manager] = Manager(
        table="gold_training",
        columns=TRAINING_COLUMNS,
        row_processor=_training_row_processor,
        save_handler=lambda manager, doc, **kwargs: _save_training_doc(manager, doc, **kwargs),
    )

    @model_validator(mode="before")
    @classmethod
    def normalize_text(cls, data):
        if not isinstance(data, dict):
            return data
        text = data.get("text")
        if isinstance(text, str):
            norm = unicodedata.normalize("NFC", text)
            data = {**data, "text": norm}
            if not data.get("text_hash"):
                data["text_hash"] = md5(norm.encode("utf-8")).hexdigest()
        return data
    
    @field_validator("spans", mode="before")
    @classmethod
    def normalize_spans(cls, v):
        """
        Accept None or [] from DB / API and normalize to [].
        This is where “span list is optional” lives.
        """
        if v is None:
            return []
        return v

    ######
    # Make certain that the text in the span 
    # matches the text in the text field 
    # and that the start and end points are 
    # set correctly
    # this is a pain, but saves later grief
    @field_validator("spans", mode="after")
    @classmethod
    def validate_span_bounds(cls, spans, info):
        text = info.data.get("text") or ""
        length = len(text)

        if not spans:
            return []

        for idx, span in enumerate(spans, start=1):
            if span.end > length:
                raise ValueError(f"Span {idx} extends past text length {length}")

            expected = text[span.start:span.end]
            if expected != span.text:
                raise ValueError(
                    f"Span {idx} text mismatch: "
                    f"got {span.text!r}, expected slice {expected!r}"
                )

        spans_sorted = sorted(spans, key=lambda s: (s.start, s.end))
        for prev, curr in zip(spans_sorted, spans_sorted[1:]):
            if curr.start < prev.end:
                raise ValueError(f"Spans {prev} and {curr} overlap")

        return spans


    @model_validator(mode="after")
    def compute_hashes(self):
        computed_text_hash = md5(self.text.encode("utf-8")).hexdigest()
        if self.text_hash and self.text_hash != computed_text_hash:
            raise ValueError("text_hash does not match normalized text")
        self.text_hash = computed_text_hash

        canonical = self.sorted_spans()
        computed_spans_hash = hashlib.sha256(
            json.dumps(canonical, separators=(",", ":")).encode("utf-8")
        ).hexdigest()
        if self.spans_hash and self.spans_hash != computed_spans_hash:
            raise ValueError("spans_hash does not match canonical spans")
        self.spans_hash = computed_spans_hash
        return self

    def sorted_spans(self) -> list[dict[str, int | str]]:
        spans_sorted = sorted(self.spans, key=lambda s: (s.start, s.end, s.label))
        return [
            {
                "start": int(span.start),
                "end": int(span.end),
                "label": span.label,
                "text": span.text,
            }
            for span in spans_sorted
        ]

    def to_record(self, *, source: Optional[str] = None, from_file: Optional[str] = None) -> dict:
        if not self.id:
            self.id = f"manual:{uuid.uuid4().hex[:12]}"
        source_value = source or self.source or "manual"
        self.source = source_value
        if from_file is not None:
            self.from_file = from_file

        record = {
            "id": self.id,
            "text": self.text,
            "text_hash": self.text_hash,
            "spans": Json(self.sorted_spans()),
            "spans_hash": self.spans_hash,
            "source": source_value,
            "from_file": self.from_file,
        }
        return record


def _save_training_doc(manager, doc, *, source: Optional[str] = None, from_file: Optional[str] = None):
    if not isinstance(doc, TrainingDoc):
        doc = TrainingDoc.model_validate(doc)

    record = doc.to_record(source=source, from_file=from_file)

    result = db.save_training_record(record, dsn=manager.dsn_value)
    result["doc"] = doc
    return result


# sanity check
if __name__ == "__main__":
    # try loading a candidate doc
    data = {"text":"I heard thus. Once the Blessed One, while wandering in the Kosala country",
            "entities": [["PERSON", "Blessed One"], ["GPE", "Kosala"]] }
    

    doc = CandidateDoc.model_validate(data)
    print(doc.model_dump())


    # try loading a training doc
    record = {
            "text": "This Makkhali Gosala...",
        "spans": [
            {"start": 5, "end": 20, "label": "PERSON", "text": "Makkhali Gosala"}
        ]
    }
    doc = TrainingDoc.model_validate(record)
    print(doc.spans[0].label)   # PERSON
    print(doc.text)
    print(doc.model_dump())   # prints record + hash
